{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention - Qutorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math, copy, time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def smiles2int(drug):\n",
    "\n",
    "    return [VOCAB_LIGAND_ISO[s] for s in drug]\n",
    "\n",
    "def seqs2int(target):\n",
    "\n",
    "    return [VOCAB_PROTEIN[s] for s in target] \n",
    "\"\"\"\n",
    "\n",
    "def rx(phi):\n",
    "    \"\"\"Single-qubit rotation for operator sigmax with angle phi.\n",
    "    -------\n",
    "    result : torch.tensor for operator describing the rotation.\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.cat((torch.cos(phi / 2).unsqueeze(dim = 0), -1j * torch.sin(phi / 2).unsqueeze(dim = 0), \n",
    "                      -1j * torch.sin(phi / 2).unsqueeze(dim = 0), torch.cos(phi / 2).unsqueeze(dim = 0)),dim = 0).reshape(2,-1)\n",
    "    # return torch.tensor([[torch.cos(phi / 2), -1j * torch.sin(phi / 2)],\n",
    "    #              [-1j * torch.sin(phi / 2), torch.cos(phi / 2)]])\n",
    "\n",
    "def ry(phi):\n",
    "    \"\"\"Single-qubit rotation for operator sigmay with angle phi.\n",
    "    -------\n",
    "    result : torch.tensor for operator describing the rotation.\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.cat((torch.cos(phi / 2).unsqueeze(dim = 0), -1 * torch.sin(phi / 2).unsqueeze(dim = 0), \n",
    "                      torch.sin(phi / 2).unsqueeze(dim = 0), torch.cos(phi / 2).unsqueeze(dim = 0)), dim = 0).reshape(2,-1) + 0j\n",
    "    # return torch.tensor([[torch.cos(phi / 2), -torch.sin(phi / 2)],\n",
    "    #              [torch.sin(phi / 2), torch.cos(phi / 2)]])\n",
    "    \n",
    "def rz(phi):\n",
    "    \"\"\"Single-qubit rotation for operator sigmaz with angle phi.\n",
    "    -------\n",
    "    result : torch.tensor for operator describing the rotation.\n",
    "    \"\"\"\n",
    "    return torch.cat((torch.exp(-1j * phi / 2).unsqueeze(dim = 0), torch.zeros(1), \n",
    "                      torch.zeros(1), torch.exp(1j * phi / 2).unsqueeze(dim = 0)), dim = 0).reshape(2,-1)    \n",
    "    # return torch.tensor([[torch.exp(-1j * phi / 2), 0],\n",
    "    #              [0, torch.exp(1j * phi / 2)]])\n",
    "\n",
    "def x_gate():\n",
    "    \"\"\"\n",
    "    Pauli x\n",
    "    \"\"\"\n",
    "    return torch.tensor([[0, 1], [1, 0]]) + 0j\n",
    "\n",
    "def y_gate():\n",
    "    \"\"\"\n",
    "    Pauli y\n",
    "    \"\"\"\n",
    "    return torch.tensor([[0, 0-1j], [0+1j, 0]])\n",
    "\n",
    "def z_gate():\n",
    "    \"\"\"\n",
    "    Pauli z\n",
    "    \"\"\"\n",
    "    return torch.tensor([[1, 0], [0, -1]]) + 0j\n",
    "\n",
    "def cnot():\n",
    "    \"\"\"\n",
    "    torch.tensor representing the CNOT gate.\n",
    "    control=0, target=1\n",
    "    \"\"\"\n",
    "    return torch.tensor([[1, 0, 0, 0],\n",
    "                 [0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1],\n",
    "                 [0, 0, 1, 0]]) + 0j\n",
    "\n",
    "def Hcz():\n",
    "    \"\"\"\n",
    "    controlled z gate for measurement\n",
    "    \"\"\"\n",
    "    return torch.tensor([[1, 0, 0, 0],\n",
    "                 [0, 1, 0, 0],\n",
    "                 [0, 0, 1, 0],\n",
    "                 [0, 0, 0, -1]]) + 0j  \n",
    "\n",
    "def rxx(phi):\n",
    "    \"\"\"\n",
    "    torch.tensor representing the rxx gate with angle phi.\n",
    "    \"\"\"\n",
    "    return torch.kron(rx(phi), rx(phi))\n",
    "\n",
    "def ryy(phi):\n",
    "    \"\"\"\n",
    "    torch.tensor representing the ryy gate with angle phi.\n",
    "    \"\"\"\n",
    "    return torch.kron(ry(phi), ry(phi))\n",
    "\n",
    "def rzz(phi):\n",
    "    \"\"\"\n",
    "    torch.tensor representing the rzz gate with angle phi.\n",
    "    \"\"\"\n",
    "    return torch.kron(rz(phi), rz(phi))\n",
    "\n",
    "\n",
    "def dag(x):\n",
    "    \"\"\"\n",
    "    compute conjugate transpose of input matrix\n",
    "    \"\"\"\n",
    "    x_conj = torch.conj(x)\n",
    "    x_dag = x_conj.permute(1, 0)\n",
    "    return x_dag\n",
    "\n",
    "def multi_kron(x_list):\n",
    "    \"\"\"\n",
    "    kron the data in the list in order\n",
    "    \"\"\"\n",
    "    x_k = torch.ones(1)\n",
    "    for x in x_list:\n",
    "        x_k = torch.kron(x_k, x)\n",
    "    return x_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gate_control(U,N,control,target):\n",
    "    if N<1:\n",
    "        raise ValueError(\"integer N must be larger or equal to 1\")\n",
    "    if control >= N:\n",
    "        raise ValueError(\"control must be integer < integer N\")\n",
    "    if target >= N:\n",
    "        raise ValueError(\"target must be integer < integer N\")\n",
    "    if target==control:\n",
    "        raise ValueError(\"control cannot be equal to target\")\n",
    "        \n",
    "    zero_zero=torch.tensor([[1, 0],[0, 0]]) + 0j\n",
    "    one_one=torch.tensor([[0, 0],[0, 1]]) + 0j\n",
    "    list1=[torch.eye(2)]*N\n",
    "    list2=[torch.eye(2)]*N\n",
    "    list1[control]=zero_zero\n",
    "    list2[control]=one_one\n",
    "    list2[target]=U\n",
    "    \n",
    "    return multi_kron(list1)+multi_kron(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gate_expand_1toN(U, N, target):\n",
    "    \"\"\"\n",
    "    representing a one-qubit gate that act on a system with N qubits.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if N < 1:\n",
    "        raise ValueError(\"integer N must be larger or equal to 1\")\n",
    "\n",
    "    if target >= N:\n",
    "        raise ValueError(\"target must be integer < integer N\")\n",
    "\n",
    "    return multi_kron([torch.eye(2)]* target + [U] + [torch.eye(2)] * (N - target - 1))\n",
    "\n",
    "def gate_expand_2toN(U, N, targets):\n",
    "    \"\"\"\n",
    "    representing a two-qubit gate that act on a system with N qubits.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if N < 2:\n",
    "        raise ValueError(\"integer N must be larger or equal to 2\")\n",
    "\n",
    "    if targets[1] >= N:\n",
    "        raise ValueError(\"target must be integer < integer N\")\n",
    "\n",
    "    return multi_kron([torch.eye(2)]* targets[0] + [U] + [torch.eye(2)] * (N - targets[1] - 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gate_sequence_product(U_list, n_qubits, left_to_right=True):\n",
    "    \"\"\"\n",
    "    Calculate the overall unitary matrix for a given list of unitary operations.\n",
    "    return: Unitary matrix corresponding to U_list.\n",
    "    \"\"\"\n",
    "\n",
    "    U_overall = torch.eye(2 ** n_qubits, 2 **  n_qubits) + 0j\n",
    "    for U in U_list:\n",
    "        if left_to_right:\n",
    "            U_overall = U @ U_overall\n",
    "        else:\n",
    "            U_overall = U_overall @ U\n",
    "\n",
    "    return U_overall\n",
    "\n",
    "def gate_sequence_product(U_list, n_qubits, left_to_right=True):\n",
    "    \"\"\"\n",
    "    Calculate the overall unitary matrix for a given list of unitary operations.\n",
    "    return: Unitary matrix corresponding to U_list.\n",
    "    \"\"\"\n",
    "\n",
    "    U_overall = torch.eye(2 ** n_qubits, 2 **  n_qubits) + 0j\n",
    "    for U in U_list:\n",
    "        if left_to_right:\n",
    "            U_overall = U @ U_overall\n",
    "        else:\n",
    "            U_overall = U_overall @ U\n",
    "\n",
    "    return U_overall\n",
    "\n",
    "def ptrace(rhoAB, dimA, dimB):\n",
    "    \"\"\"\n",
    "    rhoAB : density matrix\n",
    "    dimA: n_qubits A keep\n",
    "    dimB: n_qubits B trash\n",
    "    \"\"\"\n",
    "    mat_dim_A = 2**dimA\n",
    "    mat_dim_B = 2**dimB\n",
    "\n",
    "    id1 = torch.eye(mat_dim_A, requires_grad=True) + 0.j\n",
    "    id2 = torch.eye(mat_dim_B, requires_grad=True) + 0.j\n",
    "\n",
    "    pout = 0\n",
    "    for i in range(mat_dim_B):\n",
    "        p = torch.kron(id1, id2[i]) @ rhoAB @ torch.kron(id1, id2[i].reshape(mat_dim_B, 1))\n",
    "        pout += p\n",
    "    return pout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expecval_ZI(state, nqubit, target):\n",
    "    \"\"\"\n",
    "    state为nqubit大小的密度矩阵，target为z门放置位置\n",
    "    \n",
    "    \"\"\"\n",
    "    zgate=z_gate()\n",
    "    H = gate_expand_1toN(zgate, nqubit, target)\n",
    "    expecval = (state @ H).trace() #[-1,1]\n",
    "    expecval_real = (expecval.real + 1) / 2 #[0,1]\n",
    "    \n",
    "    return expecval_real\n",
    "\n",
    "def measure(state, nqubit):\n",
    "    \"\"\"\n",
    "    测量nqubit次期望\n",
    "    \n",
    "    \"\"\"\n",
    "    measure = torch.zeros(nqubit, 1)\n",
    "    for i in range(nqubit):\n",
    "        measure[i] = expecval_ZI(state, nqubit, list(range(nqubit))[i])\n",
    "\n",
    "    return measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(x):\n",
    "    \"\"\"\n",
    "    input: n*n matrix\n",
    "    perform L2 regularization on x, x为complex\n",
    "    \"\"\"\n",
    "    \n",
    "    # if x.norm() != 1 :\n",
    "    #     # print('l2norm:', x.norm())\n",
    "    #     x = x / (x.norm() + 1e-10)\n",
    "    # x = x.type(dtype=torch.complex64)\n",
    "    # return x\n",
    "    # from sklearn.preprocessing import normalize\n",
    "    # xn = normalize(x, norm='l2', axis=0)\n",
    "    with torch.no_grad():\n",
    "        # x = x.squeeze()\n",
    "        if x.norm() != 1:\n",
    "            xd = x.diag()\n",
    "            xds = (xd.sqrt()).unsqueeze(1)\n",
    "            xdsn = xds / (xds.norm() + 1e-12)\n",
    "            xdsn2 = xdsn @ dag(xdsn)\n",
    "            xdsn2 = xdsn2.type(dtype=torch.complex64)\n",
    "        else:\n",
    "            xdsn2 = x.type(dtype=torch.complex64)\n",
    "    # if x.norm() != 1:\n",
    "    #     with torch.no_grad():\n",
    "    #         xd = x.diag()\n",
    "    #         xds = (xd.sqrt()).unsqueeze(1)\n",
    "    #         xdsn = xds / (xds.norm() + 1e-12)\n",
    "    #         xdsn2 = xdsn @ dag(xdsn)\n",
    "    #         xdsn2 = xdsn2.type(dtype=torch.complex64)\n",
    "    # else:\n",
    "    #     xdsn2 = x.type(dtype=torch.complex64)\n",
    "    return xdsn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class init_cir_q(nn.Module):\n",
    "    \"\"\"初始化attn—q\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_qubits=2, \n",
    "                 gain=2 ** 0.5, use_wscale=True, lrmul=1):\n",
    "        super().__init__()\n",
    "\n",
    "        he_std = gain * 5 ** (-0.5)  # He init\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = nn.Parameter(nn.init.uniform_(torch.empty(n_qubits*3), a=0.0, b=2*np.pi) * init_std)# theta_size=5\n",
    "        \n",
    "        self.n_qubits = n_qubits\n",
    "\n",
    "\n",
    "    def queryQ(self):\n",
    "        w = self.weight * self.w_mul\n",
    "        cir = []\n",
    "        for which_q in range(0, self.n_qubits):\n",
    "            cir.append(gate_expand_1toN(rx(w[which_q*3+0]), self.n_qubits, which_q))\n",
    "            cir.append(gate_expand_1toN(ry(w[which_q*3+1]), self.n_qubits, which_q))        \n",
    "            cir.append(gate_expand_1toN(rz(w[which_q*3+2]), self.n_qubits, which_q))\n",
    "        #for which_q in range(0, self.n_qubits, 2):\n",
    "            #cir.append(gate_expand_1toN(rx(w[0]), self.n_qubits, which_q))\n",
    "            #cir.append(gate_expand_1toN(rx(w[1]), self.n_qubits, which_q + 1))\n",
    "            #cir.append(gate_expand_2toN(ryy(w[2]), self.n_qubits, [which_q, which_q + 1]))\n",
    "            #cir.append(gate_expand_1toN(ry(w[2]), self.n_qubits, which_q))        \n",
    "            #cir.append(gate_expand_1toN(ry(w[3]), self.n_qubits, which_q + 1))\n",
    "            #cir.append(gate_expand_1toN(rz(w[4]), self.n_qubits, which_q))        \n",
    "            #cir.append(gate_expand_1toN(rz(w[5]), self.n_qubits, which_q + 1))\n",
    "        U = gate_sequence_product(cir, self.n_qubits)\n",
    "        return U\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        E_out = self.queryQ()\n",
    "        queryQ_out = E_out@ x @ dag(E_out)\n",
    "        return queryQ_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class init_cir_k(nn.Module):\n",
    "    \"\"\"初始化attn—q\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_qubits=2, \n",
    "                 gain=2 ** 0.5, use_wscale=True, lrmul=1):\n",
    "        super().__init__()\n",
    "\n",
    "        he_std = gain * 5 ** (-0.5)  # He init\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = nn.Parameter(nn.init.uniform_(torch.empty(n_qubits*3), a=0.0, b=2*np.pi) * init_std)# theta_size=5\n",
    "        \n",
    "        self.n_qubits = n_qubits\n",
    "\n",
    "\n",
    "    def keyQ(self):\n",
    "        w = self.weight * self.w_mul\n",
    "        cir = []\n",
    "        for which_q in range(0, self.n_qubits):\n",
    "            cir.append(gate_expand_1toN(rx(w[which_q*3+0]), self.n_qubits, which_q))\n",
    "            cir.append(gate_expand_1toN(ry(w[which_q*3+1]), self.n_qubits, which_q))        \n",
    "            cir.append(gate_expand_1toN(rz(w[which_q*3+2]), self.n_qubits, which_q))\n",
    "        #for which_q in range(0, self.n_qubits, 2):\n",
    "            #cir.append(gate_expand_1toN(ry(w[0]), self.n_qubits, which_q))\n",
    "            #cir.append(gate_expand_1toN(ry(w[1]), self.n_qubits, which_q + 1))\n",
    "            ##cir.append(gate_expand_2toN(ryy(w[2]), self.n_qubits, [which_q, which_q + 1]))\n",
    "            #cir.append(gate_expand_1toN(rz(w[2]), self.n_qubits, which_q))        \n",
    "            #cir.append(gate_expand_1toN(rz(w[3]), self.n_qubits, which_q + 1))\n",
    "            #cir.append(gate_expand_1toN(rx(w[4]), self.n_qubits, which_q))        \n",
    "            #cir.append(gate_expand_1toN(rx(w[5]), self.n_qubits, which_q + 1))\n",
    "        U = gate_sequence_product(cir, self.n_qubits)\n",
    "        return U\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        E_out = self.keyQ()\n",
    "        keyQ_out = E_out @ x @ dag(E_out)\n",
    "        return keyQ_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class init_cir_v(nn.Module):\n",
    "    \"\"\"初始化attn—q\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_qubits=2, \n",
    "                 gain=2 ** 0.5, use_wscale=True, lrmul=1):\n",
    "        super().__init__()\n",
    "\n",
    "        he_std = gain * 5 ** (-0.5)  # He init\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = nn.Parameter(nn.init.uniform_(torch.empty(n_qubits*3), a=0.0, b=2*np.pi) * init_std)# theta_size=5\n",
    "        \n",
    "        self.n_qubits = n_qubits\n",
    "\n",
    "\n",
    "    def valueQ(self):\n",
    "        w = self.weight * self.w_mul\n",
    "        cir = []\n",
    "        for which_q in range(0, self.n_qubits):\n",
    "            cir.append(gate_expand_1toN(rx(w[which_q*3+0]), self.n_qubits, which_q))\n",
    "            cir.append(gate_expand_1toN(ry(w[which_q*3+1]), self.n_qubits, which_q))        \n",
    "            cir.append(gate_expand_1toN(rz(w[which_q*3+2]), self.n_qubits, which_q))\n",
    "        #for which_q in range(0, self.n_qubits, 2):\n",
    "            #cir.append(gate_expand_1toN(rz(w[0]), self.n_qubits, which_q))\n",
    "            #cir.append(gate_expand_1toN(rz(w[1]), self.n_qubits, which_q + 1))\n",
    "            ##cir.append(gate_expand_2toN(ryy(w[2]), self.n_qubits, [which_q, which_q + 1]))\n",
    "            #cir.append(gate_expand_1toN(rx(w[2]), self.n_qubits, which_q))        \n",
    "            #cir.append(gate_expand_1toN(rx(w[3]), self.n_qubits, which_q + 1))\n",
    "            #cir.append(gate_expand_1toN(ry(w[4]), self.n_qubits, which_q))        \n",
    "            #cir.append(gate_expand_1toN(ry(w[5]), self.n_qubits, which_q + 1))\n",
    "        U = gate_sequence_product(cir, self.n_qubits)\n",
    "        return U\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        E_out = self.valueQ()\n",
    "        valueQ_out = E_out @ x @ dag(E_out)\n",
    "        return valueQ_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_query_key(queryQ_out, keyQ_out, dim_q, dim_k):\n",
    "    \"\"\"queryQ_out: type torch.Tensor\n",
    "       keyQ_out: torch.Tensor\n",
    "    \"\"\"\n",
    "    out = torch.kron(queryQ_out, keyQ_out)\n",
    "    n_qubits = dim_q + dim_k\n",
    "    \n",
    "    U_list=[]\n",
    "    for t in range(dim_k):\n",
    "        U_list.append(gate_control(x_gate(),n_qubits,t,n_qubits-dim_k+t))\n",
    "    U_overall=gate_sequence_product(U_list, n_qubits)\n",
    "    \n",
    "    out=U_overall @ out @ dag(U_overall)\n",
    "    \n",
    "    quantum_score = measure(out, n_qubits)\n",
    "    \n",
    "    return quantum_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_src_value(quantum_src, valueQ_out, dim_s, dim_v):\n",
    "    \"\"\"input torch.Tensor\n",
    "    \"\"\"\n",
    "    src=quantum_src.mean()\n",
    "    phi=(src-0.5)*2*np.pi #phi=[-pi,pi]\n",
    "    \n",
    "    U_list=[]\n",
    "    ux=rx(phi*0.5)\n",
    "    uy=ry(phi*0.5)\n",
    "    uz=rz(phi)\n",
    "    for i in range(dim_v):\n",
    "        U_list.append(gate_expand_1toN(ux, dim_v, i))\n",
    "        U_list.append(gate_expand_1toN(uy, dim_v, i))\n",
    "        U_list.append(gate_expand_1toN(uz, dim_v, i))\n",
    "    \n",
    "    U_overall=gate_sequence_product(U_list,dim_v)\n",
    "    quantum_weighted_value = U_overall @ valueQ_out @ dag(U_overall)\n",
    "    \n",
    "    return quantum_weighted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_output(qwv_list, dim):\n",
    "    \n",
    "    out = multi_kron(qwv_list)\n",
    "    n_qubits=len(qwv_list)*dim\n",
    "    U_list=[]\n",
    "    for i in range(len(qwv_list)-1):\n",
    "        for t in range(dim):\n",
    "            U_list.append(gate_control(x_gate(),n_qubits,i*dim+t,n_qubits-dim+t))\n",
    "            \n",
    "    U_overall=gate_sequence_product(U_list, n_qubits)\n",
    "    \n",
    "    out=U_overall @ out @ dag(U_overall)\n",
    "        \n",
    "    attnQ = ptrace(out, dim, n_qubits-dim)\n",
    "    return attnQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    #\"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_attention(query, key, value, mask=None, dropout=None):\n",
    "    #\"Compute 'Scaled Dot Product Attention'\"\n",
    "    query_input=query.squeeze(0)\n",
    "    key_input=key.squeeze(0)\n",
    "    value_input=value.squeeze(0)\n",
    "    #print(query_input.size(-1))\n",
    "    n_qubits=math.ceil(math.log2(query_input.size(-1)))\n",
    "    #print(n_qubits)\n",
    "    \n",
    "    qqs=[]\n",
    "    qks=[]\n",
    "    qvs=[]\n",
    "\n",
    "    for x in query_input.chunk(query_input.size(-2),0):\n",
    "        #expand to 2**n_qubits length vector\n",
    "        qx=nn.ZeroPad2d((0,2**n_qubits-query_input.size(-1),0,0))(x)\n",
    "        #l2-regularization\n",
    "        qinput=encoding(qx.T@qx)\n",
    "        #print(qinput)\n",
    "        init_q=init_cir_q(n_qubits=n_qubits)\n",
    "        qqs.append(init_q(qinput))\n",
    "        \n",
    "    for x in key_input.chunk(key_input.size(-2),0):\n",
    "        #expand to 2**n_qubits length vector\n",
    "        qx=nn.ZeroPad2d((0,2**n_qubits-key_input.size(-1),0,0))(x)\n",
    "        #l2-regularization\n",
    "        qinput=encoding(qx.T@qx)\n",
    "        #print(qinput)\n",
    "        init_k=init_cir_k(n_qubits=n_qubits)\n",
    "        qks.append(init_k(qinput))\n",
    "        \n",
    "    for x in value_input.chunk(value_input.size(-2),0):\n",
    "        #expand to 2**n_qubits length vector\n",
    "        qx=nn.ZeroPad2d((0,2**n_qubits-query_input.size(-1),0,0))(x)\n",
    "        #l2-regularization\n",
    "        qinput=encoding(qx.T@qx)\n",
    "        #print(qinput)\n",
    "        init_v=init_cir_v(n_qubits=n_qubits)\n",
    "        qvs.append(init_v(qinput))\n",
    "    \n",
    "    outputs=[]\n",
    "    for i in range(len(qqs)):\n",
    "        qwvs_i=[]\n",
    "        for j in range(len(qks)):\n",
    "            score_ij=cal_query_key(qqs[i],qks[j],n_qubits,n_qubits)\n",
    "            qwvs_i.append(cal_src_value(score_ij,qvs[j],n_qubits,n_qubits))\n",
    "        out_i=measure(cal_output(qwvs_i,n_qubits),n_qubits).squeeze().unsqueeze(0)\n",
    "        outputs.append(out_i)\n",
    "        #print(out_i)\n",
    "    \n",
    "    return torch.cat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(Q_MultiHeadedAttention, self).__init__()\n",
    "        #assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linear = nn.Linear((d_model+1)//2,d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # Apply attention on all the projected vectors in batch. \n",
    "        x = q_attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        #print(x)\n",
    "        x=x.unsqueeze(0)\n",
    "        #print(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1=torch.Tensor([1,0,2,0,1,0])\n",
    "input2=torch.Tensor([0,1,1,0,1,2])\n",
    "input3=torch.Tensor([0,2,1,1,1,0])\n",
    "input4=torch.Tensor([[[1,0,2,0,1,0],[0,1,1,0,1,2],[0,2,1,1,1,0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=input4.squeeze(0)\n",
    "query.size(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-027161744bf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mqx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZeroPad2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_qubits\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mqinput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mqx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#print(qinput)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "query=input4.squeeze(0)\n",
    "n_qubits=math.ceil(math.log2(query.size(-1)))\n",
    "\n",
    "qqs=[]\n",
    "qks=[]\n",
    "qvs=[]\n",
    "\n",
    "for q in query.chunk(input4.size(-2),0):\n",
    "    qx=nn.ZeroPad2d((0,n_qubits-query.size(-1),0,0))(x)\n",
    "    qinput=encoding(qx.T@qx)\n",
    "    #print(qinput)\n",
    "    \n",
    "    init_q=init_cir_q(n_qubits=n_qubits)\n",
    "    init_k=init_cir_k(n_qubits=n_qubits)\n",
    "    init_v=init_cir_v(n_qubits=n_qubits)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=nn.ZeroPad2d((0,8-6,0,0))(x)\n",
    "y=nn.ZeroPad2d((0,8-6,0,0))(y)\n",
    "z=nn.ZeroPad2d((0,8-6,0,0))(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qin1=x.T@x\n",
    "qin2=y.T@y\n",
    "qin3=z.T@z\n",
    "#qin4=w.T@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qin1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qinput1 = encoding(qin1)\n",
    "qinput2 = encoding(qin2)\n",
    "qinput3 = encoding(qin3)\n",
    "#qinput4 = encoding(qin4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qinput1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits=math.ceil(math.log2(qinput1.size(-1)))\n",
    "init_q=init_cir_q(n_qubits=n_qubits)\n",
    "init_k=init_cir_k(n_qubits=n_qubits)\n",
    "init_v=init_cir_v(n_qubits=n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qk1=init_k(qinput1)\n",
    "qq1=init_q(qinput1)\n",
    "qv1=init_v(qinput1)\n",
    "qk2=init_k(qinput2)\n",
    "qq2=init_q(qinput2)\n",
    "qv2=init_v(qinput2)\n",
    "qk3=init_k(qinput3)\n",
    "qq3=init_q(qinput3)\n",
    "qv3=init_v(qinput3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qinput1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding(qinput1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score11=cal_query_key(qq1,qk1,2,2)\n",
    "score12=cal_query_key(qq1,qk2,2,2)\n",
    "score13=cal_query_key(qq1,qk3,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score11.size(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwv11=cal_src_value(score11,qv1,2,2)\n",
    "qwv12=cal_src_value(score12,qv2,2,2)\n",
    "qwv13=cal_src_value(score13,qv2,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(qwv11.size(dim=0)+1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1=cal_output([qwv11,qwv12,qwv13],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure(output1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(output1.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1.trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1.size(dim=0)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    #\"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention(input4,input4,input4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    #\"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_attention(query, key, value, mask=None, dropout=None):\n",
    "    #\"Compute 'Scaled Dot Product Attention'\"\n",
    "    query_input=query.squeeze(0)\n",
    "    key_input=key.squeeze(0)\n",
    "    value_input=value.squeeze(0)\n",
    "    #print(query_input.size(-1))\n",
    "    n_qubits=math.ceil(math.log2(query_input.size(-1)))\n",
    "    #print(n_qubits)\n",
    "    \n",
    "    qqs=[]\n",
    "    qks=[]\n",
    "    qvs=[]\n",
    "\n",
    "    for x in query_input.chunk(query_input.size(-2),0):\n",
    "        #expand to 2**n_qubits length vector\n",
    "        qx=nn.ZeroPad2d((0,2**n_qubits-query_input.size(-1),0,0))(x)\n",
    "        #l2-regularization\n",
    "        qinput=encoding(qx.T@qx)\n",
    "        #print(qinput)\n",
    "        init_q=init_cir_q(n_qubits=n_qubits)\n",
    "        qqs.append(init_q(qinput))\n",
    "        \n",
    "    for x in key_input.chunk(key_input.size(-2),0):\n",
    "        #expand to 2**n_qubits length vector\n",
    "        qx=nn.ZeroPad2d((0,2**n_qubits-key_input.size(-1),0,0))(x)\n",
    "        #l2-regularization\n",
    "        qinput=encoding(qx.T@qx)\n",
    "        #print(qinput)\n",
    "        init_k=init_cir_k(n_qubits=n_qubits)\n",
    "        qks.append(init_k(qinput))\n",
    "        \n",
    "    for x in value_input.chunk(value_input.size(-2),0):\n",
    "        #expand to 2**n_qubits length vector\n",
    "        qx=nn.ZeroPad2d((0,2**n_qubits-query_input.size(-1),0,0))(x)\n",
    "        #l2-regularization\n",
    "        qinput=encoding(qx.T@qx)\n",
    "        #print(qinput)\n",
    "        init_v=init_cir_v(n_qubits=n_qubits)\n",
    "        qvs.append(init_v(qinput))\n",
    "    \n",
    "    outputs=[]\n",
    "    for i in range(len(qqs)):\n",
    "        qwvs_i=[]\n",
    "        for j in range(len(qks)):\n",
    "            score_ij=cal_query_key(qqs[i],qks[j],n_qubits,n_qubits)\n",
    "            qwvs_i.append(cal_src_value(score_ij,qvs[j],n_qubits,n_qubits))\n",
    "        out_i=measure(cal_output(qwvs_i,n_qubits),n_qubits).squeeze().unsqueeze(0)\n",
    "        outputs.append(out_i)\n",
    "        #print(out_i)\n",
    "    \n",
    "    return torch.cat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_attention(input4,input4,input4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_self_attention(input4,input4,input4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        #\"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        #\"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        print(query)\n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        #print(x.transpose(1, 2).contiguous())\n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        #print(x)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma1=MultiHeadedAttention(6,6)\n",
    "tmp1=ma1(input4,input4,input4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma2=MultiHeadedAttention(6,6)\n",
    "tmp2=ma2(tmp1,tmp1,tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(Q_MultiHeadedAttention, self).__init__()\n",
    "        #assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linear = nn.Linear((d_model+1)//2,d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # Apply attention on all the projected vectors in batch. \n",
    "        x = q_attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        #print(x)\n",
    "        x=x.unsqueeze(0)\n",
    "        #print(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qma1=Q_MultiHeadedAttention(8,6)\n",
    "qma2=Q_MultiHeadedAttention(8,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input4=input4.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=qma1(input4,input4,input4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qma2(tmp,tmp,tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
